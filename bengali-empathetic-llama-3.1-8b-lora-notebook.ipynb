{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLaMA 3.1-8B with Standard LoRA on Bengali Empathetic Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T18:56:07.239453Z",
     "iopub.status.busy": "2025-12-30T18:56:07.238750Z",
     "iopub.status.idle": "2025-12-30T18:57:37.297517Z",
     "shell.execute_reply": "2025-12-30T18:57:37.296655Z",
     "shell.execute_reply.started": "2025-12-30T18:56:07.239424Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers>=4.40.0 datasets accelerate\n",
    "!pip install -q peft>=0.10.0 bitsandbytes>=0.43.0\n",
    "!pip install -q trl>=0.8.0\n",
    "!pip install -q evaluate sacrebleu rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T18:57:37.299272Z",
     "iopub.status.busy": "2025-12-30T18:57:37.299028Z",
     "iopub.status.idle": "2025-12-30T18:57:37.417924Z",
     "shell.execute_reply": "2025-12-30T18:57:37.417104Z",
     "shell.execute_reply.started": "2025-12-30T18:57:37.299248Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA availabilty: True\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.8 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA availabilty: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:01:57.364652Z",
     "iopub.status.busy": "2025-12-30T19:01:57.363929Z",
     "iopub.status.idle": "2025-12-30T19:01:57.675903Z",
     "shell.execute_reply": "2025-12-30T19:01:57.675088Z",
     "shell.execute_reply.started": "2025-12-30T19:01:57.364621Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (38233, 4)\n",
      "Column Names: ['Topics', 'Question-Title', 'Questions', 'Answers']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topics</th>\n",
       "      <th>Question-Title</th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>পারিবারিক দ্বন্দ্ব</td>\n",
       "      <td>মা ও স্ত্রীর মধ্যে মতানৈক্য বৃদ্ধি</td>\n",
       "      <td>আমার স্ত্রী এবং মায়ের মধ্যে টানটান মতবিরোধ চ...</td>\n",
       "      <td>আপনি যা বর্ণনা করছেন তাকে মনোবিজ্ঞানীরা \"ত্রি...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>পদার্থের অপব্যবহার, আসক্তি</td>\n",
       "      <td>আমি ধূমপানে আসক্ত। আমি কিভাবে থামাতে পারি?</td>\n",
       "      <td>আমি বাচ্চা নেওয়ার পরিকল্পনা করছি, তাই আমাকে ...</td>\n",
       "      <td>হাই। আপনার শিশুর (এবং নিজের) জন্য যা স্বাস্থ্...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Topics                              Question-Title  \\\n",
       "0          পারিবারিক দ্বন্দ্ব          মা ও স্ত্রীর মধ্যে মতানৈক্য বৃদ্ধি   \n",
       "1  পদার্থের অপব্যবহার, আসক্তি  আমি ধূমপানে আসক্ত। আমি কিভাবে থামাতে পারি?   \n",
       "\n",
       "                                           Questions  \\\n",
       "0   আমার স্ত্রী এবং মায়ের মধ্যে টানটান মতবিরোধ চ...   \n",
       "1   আমি বাচ্চা নেওয়ার পরিকল্পনা করছি, তাই আমাকে ...   \n",
       "\n",
       "                                             Answers  \n",
       "0   আপনি যা বর্ণনা করছেন তাকে মনোবিজ্ঞানীরা \"ত্রি...  \n",
       "1   হাই। আপনার শিশুর (এবং নিজের) জন্য যা স্বাস্থ্...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "DATASET_PATH = \"/kaggle/input/dtaset/BengaliEmpatheticConversationsCorpus .csv\"\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH, encoding='utf-8')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Column Names: {df.columns.tolist()}\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:02:00.740624Z",
     "iopub.status.busy": "2025-12-30T19:02:00.739880Z",
     "iopub.status.idle": "2025-12-30T19:02:01.755362Z",
     "shell.execute_reply": "2025-12-30T19:02:01.754775Z",
     "shell.execute_reply.started": "2025-12-30T19:02:00.740595Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid samples: 33731\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def format_for_llama(row):\n",
    "    topic = clean_text(row['Topics'])\n",
    "    question_title = clean_text(row['Question-Title'])\n",
    "    question = clean_text(row['Questions'])\n",
    "    answer = clean_text(row['Answers'])\n",
    "    \n",
    "    system_msg = f\"আপনি একজন সহানুভূতিশীল বাংলা কথোপকথন সহকারী। বিষয়: {topic}\"\n",
    "    \n",
    "    formatted = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question_title}\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{answer}<|eot_id|><|end_of_text|>\"\"\"\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "df_clean = df.dropna(subset=['Questions', 'Answers'])\n",
    "df_clean = df_clean[df_clean['Questions'].str.len() > 10]\n",
    "df_clean = df_clean[df_clean['Answers'].str.len() > 20]\n",
    "df_clean['text'] = df_clean.apply(format_for_llama, axis=1)\n",
    "print(f\"Number of valid samples: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:02:05.067378Z",
     "iopub.status.busy": "2025-12-30T19:02:05.067084Z",
     "iopub.status.idle": "2025-12-30T19:02:05.326159Z",
     "shell.execute_reply": "2025-12-30T19:02:05.325628Z",
     "shell.execute_reply.started": "2025-12-30T19:02:05.067335Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4000 | Val: 300 | Test: 300\n"
     ]
    }
   ],
   "source": [
    "MAX_CHAR_LENGTH = 5000  \n",
    "MAX_TRAIN_SAMPLES = 4000\n",
    "MAX_VAL_SAMPLES = 300\n",
    "MAX_TEST_SAMPLES = 300\n",
    "\n",
    "df_filtered = df_clean[df_clean['text'].str.len() < MAX_CHAR_LENGTH].copy()\n",
    "\n",
    "df_shuffled = df_filtered.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_df = df_shuffled[:MAX_TRAIN_SAMPLES]\n",
    "val_df = df_shuffled[MAX_TRAIN_SAMPLES:MAX_TRAIN_SAMPLES + MAX_VAL_SAMPLES]\n",
    "test_df = df_shuffled[MAX_TRAIN_SAMPLES + MAX_VAL_SAMPLES:MAX_TRAIN_SAMPLES + MAX_VAL_SAMPLES + MAX_TEST_SAMPLES]\n",
    "\n",
    "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "\n",
    "def save_jsonl(df, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for _, row in df.iterrows():\n",
    "            f.write(json.dumps({'text': row['text']}, ensure_ascii=False) + '\\n')\n",
    "\n",
    "save_jsonl(train_df, 'train.jsonl')\n",
    "save_jsonl(val_df, 'val.jsonl')\n",
    "save_jsonl(test_df, 'test.jsonl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model with QLoRA (4-bit Quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:02:08.145270Z",
     "iopub.status.busy": "2025-12-30T19:02:08.144997Z",
     "iopub.status.idle": "2025-12-30T19:05:08.353431Z",
     "shell.execute_reply": "2025-12-30T19:05:08.352813Z",
     "shell.execute_reply.started": "2025-12-30T19:02:08.145249Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-30 19:02:16.191535: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767121336.420240      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767121336.480376      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: NousResearch/Meta-Llama-3.1-8B-Instruct\n",
      "Max sequence length: 4096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da596aca960f4576ac49fe4261177cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f34fd5479a0490fa274d64c257a76f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e83b4a094d4714b96e244c18fb25ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14331564f57346e98e6a31d3f1e4a6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6d17b1f9fd41f0aff329d782c83eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47385d18753466d9b2a6ffc9784f42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd24140c8d041ee9866ef6a9a4f1952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cebdcb5dee4029b8c3459c821d5da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccdd5454f7540ae8008f2f85ea425cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56474261ec8c40719f524062854f59e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f7b234640b412ba6d9b44c5349ba3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48bf5254fd4f44189eaba9ac7cdf6686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Configuration - Using NousResearch (open access model)\n",
    "MODEL_NAME = \"NousResearch/Meta-Llama-3.1-8B-Instruct\"\n",
    "MAX_SEQ_LENGTH = 4096 \n",
    "print(f\"Model Name: {MODEL_NAME}\")\n",
    "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:05:08.356167Z",
     "iopub.status.busy": "2025-12-30T19:05:08.355574Z",
     "iopub.status.idle": "2025-12-30T19:05:08.532937Z",
     "shell.execute_reply": "2025-12-30T19:05:08.532345Z",
     "shell.execute_reply.started": "2025-12-30T19:05:08.356148Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  \n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"v_proj\",  \n",
    "    ],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:05:08.534043Z",
     "iopub.status.busy": "2025-12-30T19:05:08.533771Z",
     "iopub.status.idle": "2025-12-30T19:05:09.537563Z",
     "shell.execute_reply": "2025-12-30T19:05:09.536755Z",
     "shell.execute_reply.started": "2025-12-30T19:05:08.534019Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751ce226944a41d5b63625df648f5865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda303f9ec4d4b22a6a4a2cbb3ab3d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4000 | Eval: 300\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='train.jsonl', split='train')\n",
    "eval_dataset = load_dataset('json', data_files='val.jsonl', split='train')\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} | Eval: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:05:18.203760Z",
     "iopub.status.busy": "2025-12-30T19:05:18.203472Z",
     "iopub.status.idle": "2025-12-30T19:05:18.239178Z",
     "shell.execute_reply": "2025-12-30T19:05:18.238348Z",
     "shell.execute_reply.started": "2025-12-30T19:05:18.203738Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4000\n",
      "Max sequence length: 4096\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "OUTPUT_DIR = \"./llama_bangla_lora\"\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    dataset_text_field=\"text\",\n",
    "    \n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    \n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
    "    \n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,\n",
    "    \n",
    "    logging_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=250,\n",
    "    eval_strategy=\"no\",\n",
    "    \n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Training samples: 4000\")\n",
    "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:05:23.363238Z",
     "iopub.status.busy": "2025-12-30T19:05:23.362956Z",
     "iopub.status.idle": "2025-12-30T19:05:26.759857Z",
     "shell.execute_reply": "2025-12-30T19:05:26.759256Z",
     "shell.execute_reply.started": "2025-12-30T19:05:23.363219Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a6a26bc2db4b17b31b6218df2b7025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488e405f192743909f89d51ea0034565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119146d9ca364b328d5f4340ba199d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa8e767e659480f9c64bf43467e22b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899a920507994cb2a0ce019b99a379cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ddd889bfb0478683f17a756a1a5463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=training_args,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:05:29.633551Z",
     "iopub.status.busy": "2025-12-30T19:05:29.633019Z",
     "iopub.status.idle": "2025-12-30T19:05:29.638921Z",
     "shell.execute_reply": "2025-12-30T19:05:29.638079Z",
     "shell.execute_reply.started": "2025-12-30T19:05:29.633525Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Type: Tesla T4\n",
      "Memory reserved: 3.801 GB / 14.741 GB\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU Type: {gpu_stats.name}\")\n",
    "print(f\"Memory reserved: {start_gpu_memory} GB / {max_memory} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear memory before training to prevent OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:05:38.250004Z",
     "iopub.status.busy": "2025-12-30T19:05:38.249491Z",
     "iopub.status.idle": "2025-12-30T19:05:38.769314Z",
     "shell.execute_reply": "2025-12-30T19:05:38.768740Z",
     "shell.execute_reply.started": "2025-12-30T19:05:38.249981Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory allocated: 3.00 GB\n",
      "GPU Memory reserved: 3.03 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "print(f\"GPU Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"GPU Memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T19:05:54.056690Z",
     "iopub.status.busy": "2025-12-30T19:05:54.055938Z",
     "iopub.status.idle": "2025-12-31T04:47:34.417250Z",
     "shell.execute_reply": "2025-12-31T04:47:34.416520Z",
     "shell.execute_reply.started": "2025-12-30T19:05:54.056666Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 9:41:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.966700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.593300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.552800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.524600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.518100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.514900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.505900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.505300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.494700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.491700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.492800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.475800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.490300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.469600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.497700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.470900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.485800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.484800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T04:47:34.418724Z",
     "iopub.status.busy": "2025-12-31T04:47:34.418484Z",
     "iopub.status.idle": "2025-12-31T04:47:34.424278Z",
     "shell.execute_reply": "2025-12-31T04:47:34.423596Z",
     "shell.execute_reply.started": "2025-12-31T04:47:34.418706Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Statistics:\n",
      "Final loss: 0.5285\n",
      "Training runtime: 34899.86 seconds\n",
      "Samples/second: 0.12\n",
      "Peak GPU memory: 14.162 GB\n",
      "Memory for training: 10.361 GB\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_training = round(used_memory - start_gpu_memory, 3)\n",
    "\n",
    "print(f\"\\nTraining Statistics:\")\n",
    "print(f\"Average Training loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Training runtime: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Samples/second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"Peak GPU memory: {used_memory} GB\")\n",
    "print(f\"Memory for training: {used_memory_for_training} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T04:47:34.425451Z",
     "iopub.status.busy": "2025-12-31T04:47:34.425117Z",
     "iopub.status.idle": "2025-12-31T04:47:34.835357Z",
     "shell.execute_reply": "2025-12-31T04:47:34.834613Z",
     "shell.execute_reply.started": "2025-12-31T04:47:34.425430Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Path: ./llama_bangla_lora\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "config = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'max_seq_length': MAX_SEQ_LENGTH,\n",
    "    'lora_r': 16,\n",
    "    'lora_alpha': 32,\n",
    "    'training_loss': trainer_stats.training_loss,\n",
    "    'runtime_seconds': trainer_stats.metrics['train_runtime'],\n",
    "    'strategy': 'Standard LoRA/QLoRA',\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/training_config.json\", 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"Saved Path: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T04:47:34.837690Z",
     "iopub.status.busy": "2025-12-31T04:47:34.837380Z",
     "iopub.status.idle": "2025-12-31T04:47:34.847453Z",
     "shell.execute_reply": "2025-12-31T04:47:34.846740Z",
     "shell.execute_reply.started": "2025-12-31T04:47:34.837671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "def generate_response(user_input, topic=\"সাধারণ\", max_new_tokens=256):\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "আপনি একজন সহানুভূতিশীল বাংলা কথোপকথন সহকারী। বিষয়: {topic}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\" in response:\n",
    "        response = response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\n",
    "        response = response.split(\"<|eot_id|>\")[0].strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T04:47:34.848498Z",
     "iopub.status.busy": "2025-12-31T04:47:34.848245Z",
     "iopub.status.idle": "2025-12-31T04:48:47.530461Z",
     "shell.execute_reply": "2025-12-31T04:48:47.529812Z",
     "shell.execute_reply.started": "2025-12-31T04:47:34.848471Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL RESPONSES\n",
      "============================================================\n",
      "\n",
      "--- Test 1: পরীক্ষা ---\n",
      "User: আমি খুব চিন্তিত আছি আমার পরীক্ষার ফলাফল নিয়ে। কী করব বুঝতে পারছি না।\n",
      "\n",
      "Assistant: ওহ, আপনি অপেক্ষা করতে হবে. দেখুন আপনি ধারণাটি বিবেচনা করেন এবং আপনার উপর আরও ভাল পড়াশোনা করুন।\n",
      "\n",
      "--- Test 2: বন্ধুত্ব ---\n",
      "User: আমার বন্ধুর সাথে ঝগড়া হয়ে গেছে। খুব মন খারাপ।\n",
      "\n",
      "Assistant: ওহ না! কি ঘটেছে? আপনি উপলব্ধি করতে চাইলে, আমি এটা দেখতে পারি.\n",
      "\n",
      "--- Test 3: চাকরি ---\n",
      "User: আমি চাকরি হারিয়েছি। কীভাবে এগিয়ে যাব?\n",
      "\n",
      "Assistant: দুঃখিত, তাই. আমি আশা করি আপনি তাদের সাথে ফিরে যাবেন এবং তাদের অন্য কোন কাজ দিতে পারেন!\n",
      "\n",
      "--- Test 4: মানসিক স্বাস্থ্য ---\n",
      "User: আমি অনেক দিন ধরে বিষণ্ণ বোধ করছি। সাহায্য দরকার।\n",
      "\n",
      "Assistant: আমি এটা খুব খারাপ. আমি বলতে পারি আপনাকে কিছু বিষয় চিন্তা করতে বলব, যেগুলি আপনার উপর আঘাত ফেলতে পারে? আপনি যদি ইতিমধ্যে আপনার সমস্ত সম্পর্ককে নিয়ে আসতে পারেন তাহলে আমি এটা চিন্তা করতে পারি। একটি নির্দিষ্ট কারণ খু\n",
      "\n",
      "Saved to: generated_responses.json\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    {\"topic\": \"পরীক্ষা\", \"input\": \"আমি খুব চিন্তিত আছি আমার পরীক্ষার ফলাফল নিয়ে। কী করব বুঝতে পারছি না।\"},\n",
    "    {\"topic\": \"বন্ধুত্ব\", \"input\": \"আমার বন্ধুর সাথে ঝগড়া হয়ে গেছে। খুব মন খারাপ।\"},\n",
    "    {\"topic\": \"চাকরি\", \"input\": \"আমি চাকরি হারিয়েছি। কীভাবে এগিয়ে যাব?\"},\n",
    "    {\"topic\": \"মানসিক স্বাস্থ্য\", \"input\": \"আমি অনেক দিন ধরে বিষণ্ণ বোধ করছি। সাহায্য দরকার।\"},\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL RESPONSES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "generated_responses = []\n",
    "\n",
    "for i, test in enumerate(test_cases):\n",
    "    print(f\"\\n--- Test {i+1}: {test['topic']} ---\")\n",
    "    print(f\"User: {test['input']}\")\n",
    "    \n",
    "    response = generate_response(test['input'], test['topic'])\n",
    "    print(f\"\\nAssistant: {response}\")\n",
    "    \n",
    "    generated_responses.append({\n",
    "        'topic': test['topic'],\n",
    "        'input': test['input'],\n",
    "        'response': response\n",
    "    })\n",
    "\n",
    "with open('generated_responses.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(generated_responses, f, ensure_ascii=False, indent=2)\n",
    "print(\"\\nSaved to: generated_responses.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T04:48:47.531408Z",
     "iopub.status.busy": "2025-12-31T04:48:47.531142Z",
     "iopub.status.idle": "2025-12-31T04:48:50.139954Z",
     "shell.execute_reply": "2025-12-31T04:48:50.139100Z",
     "shell.execute_reply.started": "2025-12-31T04:48:47.531367Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f4f5ae14714caea0d2cdc385230200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f5d98ccc8b445a8d5dfb335e737c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3364c27ac04f4b2cab0046d0b2210822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Sample Size: 50\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "test_dataset = load_dataset('json', data_files='test.jsonl', split='train')\n",
    "\n",
    "NUM_EVAL_SAMPLES = 50\n",
    "eval_samples = test_dataset.select(range(min(NUM_EVAL_SAMPLES, len(test_dataset))))\n",
    "print(f\"Evaluation Sample Size: {len(eval_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T04:48:50.141330Z",
     "iopub.status.busy": "2025-12-31T04:48:50.140767Z",
     "iopub.status.idle": "2025-12-31T04:58:09.108955Z",
     "shell.execute_reply": "2025-12-31T04:58:09.108285Z",
     "shell.execute_reply.started": "2025-12-31T04:48:50.141311Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10/50\n",
      "Generated 20/50\n",
      "Generated 30/50\n",
      "Generated 40/50\n",
      "Generated 50/50\n",
      "\n",
      "Generated 50 responses\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for idx, sample in enumerate(eval_samples):\n",
    "    text = sample['text']\n",
    "    \n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\" in text:\n",
    "        ref = text.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\n",
    "        ref = ref.split(\"<|eot_id|>\")[0].strip()\n",
    "        references.append(ref)\n",
    "        \n",
    "        user_part = text.split(\"<|start_header_id|>user<|end_header_id|>\")[-1]\n",
    "        user_input = user_part.split(\"<|eot_id|>\")[0].strip()\n",
    "        \n",
    "        pred = generate_response(user_input, max_new_tokens=200)\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"Generated {idx + 1}/{len(eval_samples)}\")\n",
    "\n",
    "print(f\"\\nGenerated {len(predictions)} responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T04:58:09.110075Z",
     "iopub.status.busy": "2025-12-31T04:58:09.109770Z",
     "iopub.status.idle": "2025-12-31T04:58:09.294836Z",
     "shell.execute_reply": "2025-12-31T04:58:09.294028Z",
     "shell.execute_reply.started": "2025-12-31T04:58:09.110057Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "BLEU Score: 0.81\n",
      "ROUGE-1: 0.0000\n",
      "ROUGE-2: 0.0000\n",
      "ROUGE-L: 0.0000\n",
      "\n",
      "Empathy score: 3.67\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# BLEU\n",
    "refs_for_bleu = [[ref] for ref in references]\n",
    "bleu_result = bleu.compute(predictions=predictions, references=refs_for_bleu)\n",
    "print(f\"\\nBLEU Score: {bleu_result['score']:.2f}\")\n",
    "\n",
    "# ROUGE\n",
    "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
    "print(f\"ROUGE-1: {rouge_result['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {rouge_result['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_result['rougeL']:.4f}\")\n",
    "\n",
    "# Empathy Score\n",
    "EMPATHETIC_PATTERNS = [\n",
    "    \"বুঝতে পারছি\", \"বুঝি\", \"দুঃখিত\", \"কষ্ট\", \"মন খারাপ\",\n",
    "    \"পাশে আছি\", \"সাহায্য\", \"চেষ্টা\", \"সাহস\", \"আশা\",\n",
    "    \"স্বাভাবিক\", \"ঠিক আছে\"\n",
    "]\n",
    "\n",
    "empathy_scores = []\n",
    "for pred in predictions:\n",
    "    matches = sum(1 for p in EMPATHETIC_PATTERNS if p in pred)\n",
    "    score = min(100, (matches / len(EMPATHETIC_PATTERNS)) * 200)\n",
    "    empathy_scores.append(score)\n",
    "\n",
    "print(f\"\\nEmpathy score: {np.mean(empathy_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T04:58:09.296004Z",
     "iopub.status.busy": "2025-12-31T04:58:09.295765Z",
     "iopub.status.idle": "2025-12-31T04:58:09.301313Z",
     "shell.execute_reply": "2025-12-31T04:58:09.300715Z",
     "shell.execute_reply.started": "2025-12-31T04:58:09.295987Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to evaluation_results.json\n"
     ]
    }
   ],
   "source": [
    "eval_results = {\n",
    "    'model': MODEL_NAME,\n",
    "    'strategy': 'Standard LoRA/QLoRA',\n",
    "    'num_samples': len(predictions),\n",
    "    'metrics': {\n",
    "        'bleu': bleu_result['score'],\n",
    "        'rouge1': rouge_result['rouge1'],\n",
    "        'rouge2': rouge_result['rouge2'],\n",
    "        'rougeL': rouge_result['rougeL'],\n",
    "        'empathy_score': float(np.mean(empathy_scores))\n",
    "    },\n",
    "    'training_loss': trainer_stats.training_loss,\n",
    "    'training_time_seconds': trainer_stats.metrics['train_runtime'],\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('evaluation_results.json', 'w') as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "\n",
    "print(\"Saved to evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T04:58:09.304013Z",
     "iopub.status.busy": "2025-12-31T04:58:09.303494Z",
     "iopub.status.idle": "2025-12-31T04:58:09.321565Z",
     "shell.execute_reply": "2025-12-31T04:58:09.320954Z",
     "shell.execute_reply.started": "2025-12-31T04:58:09.303994Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING SUMMARY\n",
      "============================================================\n",
      "\n",
      "Model: NousResearch/Meta-Llama-3.1-8B-Instruct\n",
      "Strategy: Standard LoRA/QLoRA (4-bit)\n",
      "Max Sequence Length: 4096\n",
      "\n",
      "LoRA Configuration:\n",
      "  Rank: 16\n",
      "  Alpha: 32\n",
      "  Target modules: q, k, v, o, gate, up, down\n",
      "  Gradient checkpointing: Enabled\n",
      "\n",
      "Training Results:\n",
      "  Final loss: 0.5285\n",
      "  Training time: 581.7 minutes\n",
      "  Peak GPU memory: 14.162 GB\n",
      "\n",
      "Evaluation Metrics:\n",
      "  BLEU: 0.81\n",
      "  ROUGE-L: 0.0000\n",
      "  Empathy Score: 3.67\n",
      "\n",
      "Model saved to: ./llama_bangla_lora\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Strategy: Standard LoRA/QLoRA (4-bit)\")\n",
    "print(f\"Max Sequence Length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"\\nLoRA Configuration:\")\n",
    "print(f\"  Rank: 16\")\n",
    "print(f\"  Alpha: 32\")\n",
    "print(f\"  Target modules: q, k, v, o, gate, up, down\")\n",
    "print(f\"  Gradient checkpointing: Enabled\")\n",
    "print(f\"\\nTraining Results:\")\n",
    "print(f\"  Final loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"  Training time: {trainer_stats.metrics['train_runtime']/60:.1f} minutes\")\n",
    "print(f\"  Peak GPU memory: {used_memory} GB\")\n",
    "print(f\"\\nEvaluation Metrics:\")\n",
    "print(f\"  BLEU: {eval_results['metrics']['bleu']:.2f}\")\n",
    "print(f\"  ROUGE-L: {eval_results['metrics']['rougeL']:.4f}\")\n",
    "print(f\"  Empathy Score: {eval_results['metrics']['empathy_score']:.2f}\")\n",
    "print(f\"\\nModel saved to: {OUTPUT_DIR}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T04:59:04.608102Z",
     "iopub.status.busy": "2025-12-31T04:59:04.607803Z",
     "iopub.status.idle": "2025-12-31T04:59:08.083765Z",
     "shell.execute_reply": "2025-12-31T04:59:08.082797Z",
     "shell.execute_reply.started": "2025-12-31T04:59:04.608083Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T04:59:08.085306Z",
     "iopub.status.busy": "2025-12-31T04:59:08.084996Z",
     "iopub.status.idle": "2025-12-31T04:59:17.310202Z",
     "shell.execute_reply": "2025-12-31T04:59:17.309574Z",
     "shell.execute_reply.started": "2025-12-31T04:59:08.085274Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3735b74f2b7842c28c780d48444c8a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632eb5875a1a4edfb9ac6c086a0b33b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daedcd7f0463444ba24838f625cc79c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719a64d9f26e446483d540a12c2a6651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de069526308947c0ac95150d92e83116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44bf788bb2e40458da9c6db6de227ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8deccde6aeaa4f4b8fa43e55d5729df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcbff704ca63415584e9bf657d834947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f72e6aff51447bbb745ff5e33ea2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cf0dbfd5f249d0bbbed57ac21bc8aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Empathy Score (semantic): 19.33\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Empathetic phrases\n",
    "EMPATHETIC_PATTERNS = [\n",
    "    \"বুঝতে পারছি\", \"বুঝি\", \"দুঃখিত\", \"কষ্ট\", \"মন খারাপ\",\n",
    "    \"পাশে আছি\", \"সাহায্য\", \"চেষ্টা\", \"সাহস\", \"আশা\",\n",
    "    \"স্বাভাবিক\", \"ঠিক আছে\"\n",
    "]\n",
    "\n",
    "# Precompute embeddings for empathetic phrases\n",
    "pattern_embeddings = model.encode(EMPATHETIC_PATTERNS, convert_to_tensor=True)\n",
    "\n",
    "def compute_empathy_score(predictions, threshold=0.7):\n",
    "    scores = []\n",
    "    for pred in predictions:\n",
    "        pred_embedding = model.encode(pred, convert_to_tensor=True)\n",
    "        similarities = util.cos_sim(pred_embedding, pattern_embeddings)[0]\n",
    "        matches = (similarities > threshold).sum().item()\n",
    "        score = min(100, (matches / len(EMPATHETIC_PATTERNS)) * 200)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "empathy_score = compute_empathy_score(predictions)\n",
    "print(f\"\\nEmpathy Score (semantic): {empathy_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T04:59:17.311536Z",
     "iopub.status.busy": "2025-12-31T04:59:17.311021Z",
     "iopub.status.idle": "2025-12-31T04:59:17.316580Z",
     "shell.execute_reply": "2025-12-31T04:59:17.315809Z",
     "shell.execute_reply.started": "2025-12-31T04:59:17.311509Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality + Empathy Score: 0.320\n"
     ]
    }
   ],
   "source": [
    "def combined_score(bleu, rouge_l, empathy, \n",
    "                   w_bleu=0.25, w_rouge=0.15, w_emp=0.60):\n",
    "    # normalized empathy \n",
    "    empathy_norm = empathy / 100.0\n",
    "    return (w_bleu * bleu) + (w_rouge * rouge_l) + (w_emp * empathy_norm)\n",
    "\n",
    "score = combined_score(eval_results['metrics']['bleu'], eval_results['metrics']['rougeL'], empathy_score)\n",
    "print(f\"Quality + Empathy Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9160797,
     "sourceId": 14347142,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
